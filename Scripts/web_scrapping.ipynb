{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping of Portuguese News"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T20:22:28.717912Z",
     "iopub.status.busy": "2024-01-23T20:22:28.717274Z",
     "iopub.status.idle": "2024-01-23T20:22:31.162446Z",
     "shell.execute_reply": "2024-01-23T20:22:31.162041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /Users/freitas/Desktop/Projetos/Analysis-of-Portuguese-news/Data/news.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "import json\n",
    "\n",
    "#Load the configuration from a config.json\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "#Determine the base directory path based on the environment\n",
    "if os.getenv(\"GITHUB_ACTIONS\"):\n",
    "    repo_name = \"Analysis-of-Portuguese-news\"\n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\", repo_name))\n",
    "else:\n",
    "    base_dir = config[\"dir\"]\n",
    "\n",
    "#Define the filename to save to\n",
    "filename = os.path.join(base_dir, config[\"data\"][\"news\"])\n",
    "\n",
    "#Ensure the Data directory exists\n",
    "data_dir = os.path.dirname(filename)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving to {filename}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T20:22:31.165180Z",
     "iopub.status.busy": "2024-01-23T20:22:31.165039Z",
     "iopub.status.idle": "2024-01-23T20:22:31.861235Z",
     "shell.execute_reply": "2024-01-23T20:22:31.860903Z"
    }
   },
   "outputs": [],
   "source": [
    "# User-agent string as good practice\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36\",\n",
    "    \"From\": \"miguel_freitas_12@hotmail.com\"\n",
    "}\n",
    "\n",
    "#Extracting today's date into string\n",
    "good_morning = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#The base URL of the news site\n",
    "base_url = \"https://sicnoticias.pt\"\n",
    "\n",
    "#Endpoint of the specific section\n",
    "section_endpoint = \"\"\n",
    "\n",
    "#Combine the base URL with the endpoint of the section\n",
    "url = base_url + section_endpoint\n",
    "\n",
    "#Send a GET request to the URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "#Saving all full_urls in list form\n",
    "full_urls = []\n",
    "\n",
    "#Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Parse the content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    #Find all the article lists on the page\n",
    "    article_lists = soup.find_all(\"ul\", class_=\"list-articles\")\n",
    "\n",
    "    #Find all <a> tags within this list\n",
    "    for article_list in article_lists:\n",
    "        article_links = article_list.find_all(\"a\", href=True)\n",
    "        \n",
    "        #Extract all links inside <a> tags\n",
    "        for link in article_links:\n",
    "            href = link.get(\"href\")\n",
    "            \n",
    "            #Append to the base URL if the href is a relative URL\n",
    "            full_url = base_url + href if href.startswith(\"/\") else href\n",
    "            full_urls.append(full_url)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve content: Status code {response.status_code}\")\n",
    "\n",
    "#Filtering urls with more than 90 characters and that contain today's date, for purposeful links\n",
    "intended_urls = [url for url in full_urls if len(url) > 90 and good_morning in url]\n",
    "\n",
    "#Removing duplicates\n",
    "intended_urls = set(intended_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T20:22:31.863453Z",
     "iopub.status.busy": "2024-01-23T20:22:31.863340Z",
     "iopub.status.idle": "2024-01-23T20:22:54.606433Z",
     "shell.execute_reply": "2024-01-23T20:22:54.606109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 titles found!\n",
      "40 categories found!\n",
      "40 descriptions found!\n"
     ]
    }
   ],
   "source": [
    "#Create empty lists\n",
    "titles = []\n",
    "descriptions = []\n",
    "categories = []\n",
    "\n",
    "#Loop through each url found\n",
    "for each_link in intended_urls:\n",
    "\n",
    "    #Send a GET request to the URL of each link\n",
    "    response = requests.get(each_link)\n",
    "\n",
    "    #Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        #Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        ## Fetch news' title ##\n",
    "        #######################\n",
    "        \n",
    "        #Flag\n",
    "        title_found = False\n",
    "        \n",
    "        #Search inside <h1> tag\n",
    "        h1_tags = soup.find_all(\"h1\", class_=\"title\")\n",
    "        for h1_tag in h1_tags:\n",
    "            titles.append(h1_tag.text)\n",
    "            title_found = True\n",
    "        \n",
    "        #If nothing is found, append an empty string\n",
    "        if not title_found:\n",
    "            titles.append(\"\")\n",
    "            \n",
    "            \n",
    "        #############################    \n",
    "        ## Fetch news' description ##\n",
    "        #############################\n",
    "        \n",
    "        #Flag\n",
    "        description_found_going_once = False\n",
    "        \n",
    "        #Search for <p> tags\n",
    "        p_tags1 = soup.find_all('p', class_=\"g-article-lead lead\")\n",
    "        for p_tag in p_tags1:\n",
    "            descriptions.append(p_tag.text)\n",
    "            description_found_going_once = True\n",
    "        \n",
    "        #If news' description not in <p> tag\n",
    "        if not description_found_going_once:\n",
    "            \n",
    "            #Flag\n",
    "            description_found_going_twice = False\n",
    "            \n",
    "            #Search for <div> tags\n",
    "            div_tags = soup.find_all(\"div\", class_=\"g-article-lead lead\")\n",
    "            \n",
    "            #Search inside <div> tags\n",
    "            for div_tag in div_tags:\n",
    "                p_tags2 = div_tag.find_all(\"p\")\n",
    "\n",
    "                #Search inside <p> tags\n",
    "                for p_tag in p_tags2:\n",
    "                    span_tags = p_tag.find_all(\"span\")\n",
    "                    \n",
    "                    #Search for <span> tag\n",
    "                    #This block avoids news descriptions with bold to be misinterpreted\n",
    "                    spans = []\n",
    "                    for span_tag in span_tags:\n",
    "                        spans.append(span_tag.text)\n",
    "                        description_found_going_twice = True\n",
    "                    spans = \"\".join(spans)\n",
    "                    descriptions.append(spans)\n",
    "    \n",
    "            #If nothing is found, append an empty string\n",
    "            if not description_found_going_twice:\n",
    "                descriptions.append(\"\")\n",
    "            \n",
    "            \n",
    "        #############################    \n",
    "        ##  Fetch news' category   ##\n",
    "        #############################\n",
    "        \n",
    "        #Flag\n",
    "        category_found = False\n",
    "        \n",
    "        #Search for <p> tags\n",
    "        p_tags3 = soup.find_all(\"p\", class_=\"category\")\n",
    "        \n",
    "        #If <p> tags exist append their the first one's text\n",
    "        if p_tags3:\n",
    "            categories.append(p_tags3[0].text)\n",
    "            category_found = True\n",
    "        else:\n",
    "            categories.append(\"\")\n",
    "\n",
    "print(f\"{len(titles)} titles found!\\n{len(categories)} categories found!\\n{len(descriptions)} descriptions found!\")\n",
    "\n",
    "\n",
    "#Resolving mismatched description number if it happens\n",
    "if len(titles) != len(descriptions):\n",
    "    number_of_missing_desc = descriptions.count(\"\")\n",
    "    if number_of_missing_desc > 0:\n",
    "        for _ in range(number_of_missing_desc):\n",
    "            if \"\" in descriptions:\n",
    "                missing_desc_index = descriptions.index(\"\")\n",
    "                descriptions.pop(missing_desc_index)\n",
    "    print(f\"\\n{number_of_missing_desc} empty descriptions found! Mismatched descriptions resolved\")\n",
    "    print(f\"No of descriptions: {len(descriptions)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing data in an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T20:22:54.624756Z",
     "iopub.status.busy": "2024-01-23T20:22:54.624630Z",
     "iopub.status.idle": "2024-01-23T20:22:54.718140Z",
     "shell.execute_reply": "2024-01-23T20:22:54.717846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News from 2025-01-26 downloaded!\n",
      "40 new headlines added to the file.\n",
      "11775 headlines in total.\n"
     ]
    }
   ],
   "source": [
    "#Creating DataFrame\n",
    "news = pd.DataFrame({\"date\": good_morning, \"category\": categories, \"title\": titles, \"description\": descriptions})\n",
    "\n",
    "try:\n",
    "    #Check if the Excel file exists and has data\n",
    "    if os.path.isfile(filename):\n",
    "        \n",
    "        #Read existing data\n",
    "        existing_data = pd.read_excel(filename, sheet_name = \"Sheet1\")\n",
    "        \n",
    "        #Append new data\n",
    "        combined_data = pd.concat([existing_data, news], ignore_index=True)\n",
    "        \n",
    "        #Removing duplicates in case the program is run more than once per day\n",
    "        combined_data = combined_data.drop_duplicates()\n",
    "        \n",
    "    else:\n",
    "        #If file does not exist, use new data as the combined data\n",
    "        combined_data = news\n",
    "\n",
    "    #Write combined data back to Excel\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        combined_data.to_excel(writer, sheet_name = \"Sheet1\", index=False)\n",
    "        \n",
    "    print(f\"News from {good_morning} downloaded!\\n{len(titles)} new headlines added to the file.\\n{len(combined_data)} headlines in total.\")\n",
    "\n",
    "#Catching any errors while trying to write on the Excel file\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
